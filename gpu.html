<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPU-servers &mdash; HPC user-guides 2024 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/extra.css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Containers (Singularity &amp; Docker)" href="singularity.html" />
    <link rel="prev" title="Visualization" href="visualization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #E4067E" >

          
          
          <a href="index.html" class="icon icon-home">
            HPC user-guides
              <img src="_static/TalTech_Gradient-200px.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lumi.html">LUMI</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud.html">Quickstart: Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart: Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="learning.html">Courses and introductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Module environment (lmod)</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">Software packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpi.html">Available MPI versions (and comparison)</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiling.html">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GPU-servers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#hardware">Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="#login-and-home-on-ada">Login and $HOME on “ada*”</a></li>
<li class="toctree-l2"><a class="reference internal" href="#login-and-home-on-amp-and-amp2">Login and $HOME on “amp” and “amp2”</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-jobs">Running jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#software-and-modules">Software and modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#modules-specific-on-amp"><em>Modules specific on amp</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="#from-ai-lab"><em>From AI lab</em></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gpu-libraries-and-tools">GPU libraries and tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#nvidia-cuda-11"><em>Nvidia CUDA 11</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="#offloading-compilers"><em>Offloading Compilers</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-offloading"><em>OpenMP offloading</em></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nvidia-hpc-sdk"><em>Nvidia HPC SDK</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#openacc-offloading"><em>OpenACC offloading</em></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1"><em>Nvidia HPC SDK</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="#gcc-needs-testing"><em>GCC (needs testing)</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#hip-upcoming"><em>HIP (upcoming)</em></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#example-jobs">Example jobs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-job-for-meshroom">Example job for meshroom</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-job-for-colmap-needs-gpu-for-part-of-the-tasks">Example job for colmap (needs GPU for part of the tasks)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#singularity">Singularity</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="singularity.html">Containers (Singularity &amp; Docker)</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgement.html">Acknowledgement</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #E4067E" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">HPC user-guides</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">GPU-servers</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/gpu.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><span style="color:orange">only partially changed to rocky yet</span></p>
<section id="gpu-servers">
<h1>GPU-servers<a class="headerlink" href="#gpu-servers" title="Permalink to this heading"></a></h1>
<p><span style="color:red"> NB: The current Ubuntu version of amp/amp2, will be reinstalled with Rocky8 in August. Please test your software before on the ada nodes (job submission from base, not like currently from/for amp). </span></p>
<span style="color:blue">
Running your jobs on <b>amp</b> currently requires special procedure: you need to ssh into amp to submit jobs (and prepare envs) </span>
The reason for this are differences in the operating system (amp -- ubuntu, base -- rocky).<br>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr><section id="hardware">
<h2>Hardware<a class="headerlink" href="#hardware" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
 <div class="simple1"> <b>amp</b> <ul class="simple">
<li><p>CPU: 2x AMD EPYC 7742 64core</p></li>
<li><p>RAM: 1 TB</p></li>
<li><p>GPUs: 8x A100 Nvidia 40GB</p></li>
<li><p>OS: Ubuntu</p></li>
<li><p><span style="color:blue">job submission from “amp”</span></p></li>
</ul>
 </div> 
 <br> <div class="simple1"> <b>amp2</b> <ul class="simple">
<li><p>CPU: 2x AMD EPYC 7713 64core (3rd gen EPYC, Zen3)</p></li>
<li><p>RAM: 2 TB</p></li>
<li><p>GPUs: 8x A100 Nvidia 80GB</p></li>
<li><p>OS: Ubuntu</p></li>
<li><p><span style="color:blue">job submission from “amp”</span></p></li>
</ul>
 </div> 
 <br> <div class="simple1"> <b>ada2</b> <ul class="simple">
<li><p>CPU: 2x AMD EPYC 9354 32core (4th gen EPYC, Zen4)</p></li>
<li><p>RAM: 2 TB</p></li>
<li><p>GPUs: 2x L40 Nvidia 48GB</p></li>
<li><p>avx512</p></li>
<li><p>OS: Rocky8</p></li>
<li><p><span style="color:blue">job submission from “base”</span></p></li>
</ul>
 </div> 
 <br><br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="login-and-home-on-ada">
<h2>Login and $HOME on “ada*”<a class="headerlink" href="#login-and-home-on-ada" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>No direct login, jobs are submitted from “base”, use <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">-w</span> <span class="pre">ada2</span> <span class="pre">-p</span> <span class="pre">gpu</span> <span class="pre">--gres=gpu:L40</span> <span class="pre">--pty</span> <span class="pre">bash</span></code></p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="login-and-home-on-amp-and-amp2">
<h2>Login and $HOME on “amp” and “amp2”<a class="headerlink" href="#login-and-home-on-amp-and-amp2" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>The server shares the home directory with all cluster nodes.</p>
<p><strong>Amp</strong> cannot login directly, only through a jump host:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ssh</span>  <span class="n">Uni</span><span class="o">-</span><span class="n">ID</span><span class="nd">@amp</span> <span class="o">-</span><span class="n">J</span> <span class="n">Uni</span><span class="o">-</span><span class="n">ID</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span> 
</pre></div>
</div>
<p>The home-directory is the same as the cluster (<code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/home/&lt;uniID&gt;</span></code>), additionally the AI-Lab home directories are mounted under <code class="docutils literal notranslate"><span class="pre">/illukas/home/</span></code></p>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="running-jobs">
<h2>Running jobs<a class="headerlink" href="#running-jobs" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>Jobs for <strong>amp/amp2</strong> need to be submitted from <strong>amp/amp2</strong> (not from <strong>base)</strong> to ensure that all environment is set-up correctly.</p>
<p>Jobs need to be submitted using <code class="docutils literal notranslate"><span class="pre">srun</span></code> or <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, do not run jobs outside the batch system.</p>
<p>Interactive jobs are started using <code class="docutils literal notranslate"><span class="pre">srun</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">-</span><span class="n">p</span> <span class="n">gpu</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span> <span class="o">--</span><span class="n">pty</span> <span class="n">bash</span>
</pre></div>
</div>
<p>GPUs have to be reserved/requested with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">-</span><span class="n">p</span> <span class="n">gpu</span> <span class="o">--</span><span class="n">gres</span><span class="o">=</span><span class="n">gpu</span><span class="p">:</span><span class="n">A100</span><span class="p">:</span><span class="mi">1</span> <span class="o">-</span><span class="n">t</span> <span class="mi">1</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span> <span class="o">--</span><span class="n">pty</span> <span class="n">bash</span>
</pre></div>
</div>
<p>both <strong>amp/amp2</strong> are in the same partition (<code class="docutils literal notranslate"><span class="pre">-p</span> <span class="pre">gpu</span></code>) so jobs that do not have specific requirements can run on any of the two nodes. If you need a specific type, e.g. for testing performance or because of memory requirements:</p>
<ul class="simple">
<li><p>it is possible to request the feature “A100-40” (for the 40GB A100s in <strong>amp)</strong> or “A100-80” (for the 80GB A100s in <strong>amp2):</strong> <code class="docutils literal notranslate"><span class="pre">--constraint=A100-40</span></code></p></li>
<li><p>another option is to request the job to run on a specific node, using the <code class="docutils literal notranslate"><span class="pre">-w</span></code> switch (e.g. <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">-p</span> <span class="pre">gpu</span> <span class="pre">-w</span> <span class="pre">amp</span> <span class="pre">...</span> </code>)</p></li>
</ul>
<p>You can see which GPUs have been assigned to your job using <code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">$CUDA_VISIBLE_DEVICES</span></code>, <strong>the CUDA-deviceID in your programs always start with “0” (no matter which physical GPU was assigned to you by SLURM)</strong>.</p>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="software-and-modules">
<h2>Software and modules<a class="headerlink" href="#software-and-modules" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<section id="modules-specific-on-amp">
<h3><em>Modules specific on amp</em><a class="headerlink" href="#modules-specific-on-amp" title="Permalink to this heading"></a></h3>
<p>Enable the SPACK software modules (special for <strong>amp):</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">amp</span><span class="o">-</span><span class="n">spack</span>
</pre></div>
</div>
<p>The Nvidia HPC SDK (includes CUDA, nvcc and PGI compilers) is available with the following module directory (see below):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">amp</span>
</pre></div>
</div>
</section>
<section id="from-ai-lab">
<h3><em>From AI lab</em><a class="headerlink" href="#from-ai-lab" title="Permalink to this heading"></a></h3>
<p>Enable the modules for AI lab software from illukas:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">amp</span>
</pre></div>
</div>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
</section>
<section id="gpu-libraries-and-tools">
<h2>GPU libraries and tools<a class="headerlink" href="#gpu-libraries-and-tools" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>The GPUs installed are Nvidia A100 with compute capability 80, compatible with CUDA 11. However, when developing own software, be aware of vendor lockin, CUDA is only available for Nvidia GPUs and does not work on AMD GPUs. Some new supercomputers (LUMI (CSC), El Capitan (LLNL), Frontier (ORNL)) are using AMD, and some plan the Intel “Ponte Vecchio” GPU (Aurora (ANL), SuperMUC-NG (LRZ)). To be future-proof, portable methods like OpenACC/OpenMP are recommended.</p>
<p>Porting to AMD/HIP for LUMI: <a class="reference external" href="https://www.lumi-supercomputer.eu/preparing-codes-for-lumi-converting-cuda-applications-to-hip/">https://www.lumi-supercomputer.eu/preparing-codes-for-lumi-converting-cuda-applications-to-hip/</a></p>
<section id="nvidia-cuda-11">
<h3><em>Nvidia CUDA 11</em><a class="headerlink" href="#nvidia-cuda-11" title="Permalink to this heading"></a></h3>
<p>Again, beware of the vendor lockin.</p>
<p>To compile CUDA code, use the Nvidia compiler wrapper:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nvcc</span>
</pre></div>
</div>
</section>
<section id="offloading-compilers">
<h3><em>Offloading Compilers</em><a class="headerlink" href="#offloading-compilers" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>PGI (Nvidia HPC-SDK) supports OpenACC and OpenMP offloading to Nvidia GPUs</p></li>
<li><p>GCC-10.3.0</p></li>
<li><p>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</p></li>
<li><p>LLVM-13.0.0 (Clang/Flang) with NVPTX supports GPU-offloading using OpenMP pragmas</p></li>
</ul>
<br><p>See also: <a class="reference external" href="https://lumi-supercomputer.eu/offloading-code-with-compiler-directives/">https://lumi-supercomputer.eu/offloading-code-with-compiler-directives/</a></p>
</section>
<section id="openmp-offloading">
<h3><em>OpenMP offloading</em><a class="headerlink" href="#openmp-offloading" title="Permalink to this heading"></a></h3>
<p>Since version 4.0 supports offloading to accelerators. It can be utilized by GCC, LLVM (C/Flang) and Nvidia HPC-SDK (former PGI compilers).</p>
<ul class="simple">
<li><p>GCC-10.3.0</p></li>
<li><p>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</p></li>
<li><p>LLVM-13.0.0 (Clang/Flang) with NVPTX supports GPU-offloading using OpenMP pragmas</p></li>
<li><p>AOMP</p></li>
</ul>
<br><p>List of compiler support for OpenMP: <a class="reference external" href="https://www.openmp.org/resources/openmp-compilers-tools/">https://www.openmp.org/resources/openmp-compilers-tools/</a></p>
<p>Current recommendation: use Clang or GCC or AOMP</p>
<section id="nvidia-hpc-sdk">
<h4><em>Nvidia HPC SDK</em><a class="headerlink" href="#nvidia-hpc-sdk" title="Permalink to this heading"></a></h4>
<p>Compile option <code class="docutils literal notranslate"><span class="pre">-⁠mp</span></code> for CPU-OpenMP or <code class="docutils literal notranslate"><span class="pre">-mp=gpu</span></code> for GPU-OpenMP-offloading.</p>
<p>The table below summarizes useful compiler flags to compile you OpenMP code with offloading.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th></th>
<th>NVC/NVFortran</th>
<th>Clang/Cray/AMD</th>
<th>GCC/GFortran</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenMP flag</td>
<td>-mp</td>
<td>-fopenmp</td>
<td>-fopenmp -foffload=<target></td>
</tr>
<tr>
<td>Offload flag</td>
<td>-mp=gpu</td>
<td>-fopenmp-targets=<target></td>
<td>-foffload=<target></td>
</tr>
<tr>
<td>Target NVIDIA</td>
<td>default</td>
<td>nvptx64-nvidia-cuda</td>
<td>nvptx-none</td>
</tr>
<tr>
<td>Target AMD</td>
<td>n/a</td>
<td>amdgcn-amd-amdhsa</td>
<td>amdgcn-amdhsa</td>
</tr>
<tr>
<td>GPU Architecture</td>
<td>-gpu=<cc></td>
<td>-Xopenmp-target -march=<arch></td>
<td>-foffload=”-march=<arch></td>
</tr>
</tbody>
</table></section>
</section>
<section id="openacc-offloading">
<h3><em>OpenACC offloading</em><a class="headerlink" href="#openacc-offloading" title="Permalink to this heading"></a></h3>
<p>OpenACC is a portable compiler directive based approach to GPU computing. It can be utilized by GCC, (LLVM (C/Flang)) and Nvidia HPC-SDK (former PGI compilers).</p>
<p>Current recommendation: use HPC-SDK</p>
<section id="id1">
<h4><em>Nvidia HPC SDK</em><a class="headerlink" href="#id1" title="Permalink to this heading"></a></h4>
<p>Installed are versions 21.2, 21.5 and 21.9 (2021). These come with modulefiles, to use them, enable the the directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">amp</span>
</pre></div>
</div>
<p>then load the module you want to use, e.g.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">nvhpc</span><span class="o">-</span><span class="n">nompi</span><span class="o">/</span><span class="mf">21.5</span>
</pre></div>
</div>
<p>The HPC SDK also comes with a profiler, to identify regions that would benefit most from GPU acceleration.</p>
<p>OpenACC is based on compiler pragmas enabling an incremental approach to parallelism (you never break the sequential program), it can be used for CPUs (multicore) and GPUs (tesla).</p>
<p>Compiling an OpenACC program with the Nvidia compiler:
get accelerator information</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pgaccelinfo</span>
</pre></div>
</div>
<p>compile for multicore (C and Fortran commands)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pgcc</span> <span class="o">-</span><span class="n">fast</span> <span class="o">-</span><span class="n">ta</span><span class="o">=</span><span class="n">multicore</span> <span class="o">-</span><span class="n">Minfo</span><span class="o">=</span><span class="n">accel</span> <span class="o">-</span><span class="n">I</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">hpc_sdk</span><span class="o">/</span><span class="n">Linux_x86_64</span><span class="o">/</span><span class="mf">21.5</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="mf">11.3</span><span class="o">/</span><span class="n">targets</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">/</span><span class="n">include</span><span class="o">/</span>  <span class="o">-</span><span class="n">o</span> <span class="n">laplace</span> <span class="n">jacobi</span><span class="o">.</span><span class="n">c</span> <span class="n">laplace2d</span><span class="o">.</span><span class="n">c</span>
<span class="n">pgfortran</span> <span class="o">-</span><span class="n">fast</span> <span class="o">-</span><span class="n">ta</span><span class="o">=</span><span class="n">multicore</span>  <span class="o">-</span><span class="n">Minfo</span><span class="o">=</span><span class="n">accel</span> <span class="o">-</span><span class="n">I</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">hpc_sdk</span><span class="o">/</span><span class="n">Linux_x86_64</span><span class="o">/</span><span class="mf">21.5</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="mf">11.3</span><span class="o">/</span><span class="n">targets</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">/</span><span class="n">include</span><span class="o">/</span> <span class="o">-</span><span class="n">o</span> <span class="n">laplace_multicore</span> <span class="n">laplace2d</span><span class="o">.</span><span class="n">f90</span> <span class="n">jacobi</span><span class="o">.</span><span class="n">f90</span>
</pre></div>
</div>
<p>compile for GPU (C and Fortran commands)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pgcc</span> <span class="o">-</span><span class="n">fast</span> <span class="o">-</span><span class="n">ta</span><span class="o">=</span><span class="n">tesla</span> <span class="o">-</span><span class="n">Minfo</span><span class="o">=</span><span class="n">accel</span>  <span class="o">-</span><span class="n">I</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">hpc_sdk</span><span class="o">/</span><span class="n">Linux_x86_64</span><span class="o">/</span><span class="mf">21.5</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="mf">11.3</span><span class="o">/</span><span class="n">targets</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">/</span><span class="n">include</span><span class="o">/</span> <span class="o">-</span><span class="n">o</span> <span class="n">laplace_gpu</span> <span class="n">jacobi</span><span class="o">.</span><span class="n">c</span> <span class="n">laplace2d</span><span class="o">.</span><span class="n">c</span>
<span class="n">pgfortran</span> <span class="o">-</span><span class="n">fast</span> <span class="o">-</span><span class="n">ta</span><span class="o">=</span><span class="n">tesla</span><span class="p">,</span><span class="n">managed</span> <span class="o">-</span><span class="n">Minfo</span><span class="o">=</span><span class="n">accel</span> <span class="o">-</span><span class="n">I</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">nvidia</span><span class="o">/</span><span class="n">hpc_sdk</span><span class="o">/</span><span class="n">Linux_x86_64</span><span class="o">/</span><span class="mf">21.5</span><span class="o">/</span><span class="n">cuda</span><span class="o">/</span><span class="mf">11.3</span><span class="o">/</span><span class="n">targets</span><span class="o">/</span><span class="n">x86_64</span><span class="o">-</span><span class="n">linux</span><span class="o">/</span><span class="n">include</span><span class="o">/</span> <span class="o">-</span><span class="n">o</span> <span class="n">laplace_gpu</span> <span class="n">laplace2d</span><span class="o">.</span><span class="n">f90</span> <span class="n">jacobi</span><span class="o">.</span><span class="n">f90</span>
</pre></div>
</div>
<p>Profiling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nsys</span> <span class="n">profile</span> <span class="o">-</span><span class="n">t</span> <span class="n">nvtx</span> <span class="o">--</span><span class="n">stats</span><span class="o">=</span><span class="n">true</span> <span class="o">--</span><span class="n">force</span><span class="o">-</span><span class="n">overwrite</span> <span class="n">true</span> <span class="o">-</span><span class="n">o</span> <span class="n">laplace</span> <span class="o">./</span><span class="n">laplace</span>
<span class="n">nsys</span> <span class="n">profile</span> <span class="o">-</span><span class="n">t</span> <span class="n">openacc</span> <span class="o">--</span><span class="n">stats</span><span class="o">=</span><span class="n">true</span> <span class="o">--</span><span class="n">force</span><span class="o">-</span><span class="n">overwrite</span> <span class="n">true</span> <span class="o">-</span><span class="n">o</span> <span class="n">laplace_data_clauses</span> <span class="o">./</span><span class="n">laplace_data_clauses</span> <span class="mi">1024</span> <span class="mi">1024</span>
</pre></div>
</div>
<p>Analysing the profile using CLI:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nsys</span> <span class="n">stat</span> <span class="n">s</span> <span class="n">laplace</span><span class="o">.</span><span class="n">qdrep</span>
</pre></div>
</div>
<p>using the GUI:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nsys</span><span class="o">-</span><span class="n">ui</span>
</pre></div>
</div>
<p>then load the <code class="docutils literal notranslate"><span class="pre">.qdrep</span></code> file.</p>
</section>
<section id="gcc-needs-testing">
<h4><em>GCC (needs testing)</em><a class="headerlink" href="#gcc-needs-testing" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p>GCC-10.3.0</p></li>
<li><p>GCC-11.2.0 with NVPTX supports GPU-offloading using OpenMP and OpenACC pragmas</p></li>
</ul>
<br></section>
</section>
<section id="hip-upcoming">
<h3><em>HIP (upcoming)</em><a class="headerlink" href="#hip-upcoming" title="Permalink to this heading"></a></h3>
<p>For porting code to AMD-Instinct based LUMI, the AMD HIP SDK will be installed.</p>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
</section>
<section id="example-jobs">
<h2>Example jobs<a class="headerlink" href="#example-jobs" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<section id="example-job-for-meshroom">
<h3>Example job for meshroom<a class="headerlink" href="#example-job-for-meshroom" title="Permalink to this heading"></a></h3>
<p>** under construction **</p>
</section>
<section id="example-job-for-colmap-needs-gpu-for-part-of-the-tasks">
<h3>Example job for colmap (needs GPU for part of the tasks)<a class="headerlink" href="#example-job-for-colmap-needs-gpu-for-part-of-the-tasks" title="Permalink to this heading"></a></h3>
<p>** under construction **</p>
<p>See the AI-lab guide <a class="reference external" href="https://gitlab.cs.ttu.ee/ai-lab/samples/structure-from-motion-with-colmap">https://gitlab.cs.ttu.ee/ai-lab/samples/structure-from-motion-with-colmap</a>, which needs to be slightly adapted.</p>
<br></section>
<section id="singularity">
<h3>Singularity<a class="headerlink" href="#singularity" title="Permalink to this heading"></a></h3>
<p>The container solution <em>singularity</em> is available (can also run docker container).
Use with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span> <span class="n">load</span> <span class="n">amp</span>
<span class="n">module</span> <span class="n">load</span> <span class="n">Singularity</span>
</pre></div>
</div>
<p>pull the docker image you want, here ubuntu:18.04:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">singularity</span> <span class="n">pull</span> <span class="n">docker</span><span class="p">:</span><span class="o">//</span><span class="n">ubuntu</span><span class="p">:</span><span class="mf">18.04</span>
</pre></div>
</div>
<p>write an sbatch file (here called <code class="docutils literal notranslate"><span class="pre">ubuntu.slurm</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH -t 0-00:30</span>
<span class="c1">#SBATCH -N 1</span>
<span class="c1">#SBATCH -c 1</span>
<span class="c1">#SBATCH -p gpu</span>
<span class="c1">#SBATCH --gres=gpu:A100:1</span>
<span class="c1">#SBATCH --mem-per-cpu=4000</span>

<span class="n">singularity</span> <span class="n">exec</span> <span class="n">docker</span><span class="p">:</span><span class="o">//</span><span class="n">ubuntu</span><span class="p">:</span><span class="mf">18.04</span> <span class="n">cat</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">issue</span>
</pre></div>
</div>
<p>submit to the queueing system with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="n">ubuntu</span><span class="o">.</span><span class="n">slurm</span>
</pre></div>
</div>
<p>and when the resources become available, your job will be executed.</p>
<br>
<br></section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="visualization.html" class="btn btn-neutral float-left" title="Visualization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="singularity.html" class="btn btn-neutral float-right" title="Containers (Singularity &amp; Docker)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright COPYRIGHT.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>