<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quickstart: Cluster &mdash; HPC user-guides 2024 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/extra.css" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Courses and introductions" href="learning.html" />
    <link rel="prev" title="Quickstart: Cloud" href="cloud.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #E4067E" >

          
          
          <a href="index.html" class="icon icon-home">
            HPC user-guides
              <img src="_static/TalTech_Gradient-200px.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="lumi.html">LUMI</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud.html">Quickstart: Cloud</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart: Cluster</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#accessing-the-cluster">Accessing the cluster</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ssh-fingerprints-of-host-keys"><em>SSH fingerprints of host-keys</em></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#structure-and-file-tree">Structure and file tree</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-jobs-with-the-slurm">Running jobs with the SLURM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slurm-accounts">SLURM accounts</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monitoring-jobs-resources">Monitoring jobs &amp; resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#monitoring-a-job-on-the-node">Monitoring a job on the node</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#status-of-a-job">Status of a job</a></li>
<li class="toctree-l4"><a class="reference internal" href="#load-of-the-node">Load of the node</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-with-interactive-job">Monitoring with interactive job</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring-jobs-using-gpus">Monitoring jobs using GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#monitoring-resource-usage">Monitoring resource usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#copying-data-to-from-the-clusters">Copying data to/from the clusters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#smb-cifs-exported-filesystems">SMB/CIFS exported filesystems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#windows-access">Windows access</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linux-access">Linux access</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#special-considerations-for-copying-windows-linux">Special considerations for copying Windows - Linux</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#backup">Backup</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="learning.html">Courses and introductions</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">Module environment (lmod)</a></li>
<li class="toctree-l1"><a class="reference internal" href="software.html">Software packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="mpi.html">Available MPI versions (and comparison)</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="profiling.html">Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">GPU-servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="singularity.html">Containers (Singularity &amp; Docker)</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgement.html">Acknowledgement</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #E4067E" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">HPC user-guides</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quickstart: Cluster</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/quickstart.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p><span style="color:red">not changed to rocky yet</span></p>
<section id="quickstart-cluster">
<h1>Quickstart: Cluster<a class="headerlink" href="#quickstart-cluster" title="Permalink to this heading"></a></h1>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr><section id="accessing-the-cluster">
<h2>Accessing the cluster<a class="headerlink" href="#accessing-the-cluster" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p><strong>NB! To access the cluster, user must have an active <a class="reference external" href="https://taltech.atlassian.net/wiki/spaces/ITI/pages/38994346/Uni-ID+ehk+Digitaalne+identiteet">Uni-ID account</a>.</strong> For people who are neither students nor employees of Taltech <a class="reference external" href="https://taltech.atlassian.net/wiki/spaces/ITI/pages/38996020/Uni-ID+lepinguv+line+konto">Uni-ID non-contractual account</a> should be created by the head of a structural unit.</p>
<p><strong>To get access to HPC contact us by email (hpcsupport&#64;taltech.ee) or <a class="reference external" href="https://portal.taltech.ee/v2">Taltech portal</a></strong> (Help centre -&gt; Teadusarvutuste keskus (HPC centre)). We need the following information: uni-ID, department, project that pays the <a class="reference external" href="https://hpc.pages.taltech.ee/user-guides/index.html#billing">costs</a>.</p>
<p>The login-node of the cluster can be reached by SSH. SSH (the Secure SHell) is available using the command <code class="docutils literal notranslate"><span class="pre">ssh</span></code> in <strong>Linux/Unix, Mac</strong> and <strong>Windows-10.</strong>  A guide for Windows users using PuTTY (an alternative SSH using a graphical user interface (GUI)) is <a class="reference internal" href="putty.html"><span class="doc">here</span></a>.</p>
<p>For accessing the cluster <strong>base.hpc.taltech.ee</strong> use command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ssh</span> <span class="n">uni</span><span class="o">-</span><span class="n">ID</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>
</pre></div>
</div>
<p><em><strong>where uni-ID should be changed to user’s uni-ID.</strong></em></p>
<p>The cluster is accessible from inside the university and from major Estonian network providers. If you are traveling (or not on one of the major networks), the access requires <a class="reference external" href="https://taltech.atlassian.net/wiki/spaces/ITI/pages/38994267/Kaug+hendus+FortiClient+VPN+Remote+connection+with+FortiClient+VPN">FortiVPN</a> (with previously shown command) or a two-step login using a jump-host:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ssh</span> <span class="o">-</span><span class="n">l</span> <span class="n">uni</span><span class="o">-</span><span class="n">ID</span><span class="nd">@intra</span><span class="o">.</span><span class="n">ttu</span><span class="o">.</span><span class="n">ee</span> <span class="n">uni</span><span class="o">-</span><span class="n">ID</span><span class="nd">@proksi</span><span class="o">.</span><span class="n">intra</span><span class="o">.</span><span class="n">ttu</span><span class="o">.</span><span class="n">ee</span>
<span class="n">ssh</span> <span class="n">uni</span><span class="o">-</span><span class="n">ID</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>
</pre></div>
</div>
<p><em>where all uni-ID should be changed to user’s uni-ID.</em></p>
<p>For using graphical applications add the <code class="docutils literal notranslate"><span class="pre">-X</span></code> switch to the SSH command, and for GLX (X Window System) forwarding additionally the <code class="docutils literal notranslate"><span class="pre">-Y</span></code> switch, so to be able to start a GUI program that uses GLX the connection command would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ssh</span> <span class="o">-</span><span class="n">X</span> <span class="o">-</span><span class="n">Y</span> <span class="n">uni</span><span class="o">-</span><span class="n">ID</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>
</pre></div>
</div>
<p><em><strong>NB!</strong></em> <strong>The login-node is for some light interactive analysis. For heavy computations, request a (interactive) session on a compute node with the resource manager <a class="reference external" href="https://hpc.pages.taltech.ee/user-guides/quickstart.html#running-jobs-with-the-slurm">SLURM</a> or submit job for execution by SLURM sbatch script!</strong></p>
<p><strong>We strongly recommend to use SSH-keys for logging to the cluster.</strong></p>
<section id="ssh-fingerprints-of-host-keys">
<h3><em>SSH fingerprints of host-keys</em><a class="headerlink" href="#ssh-fingerprints-of-host-keys" title="Permalink to this heading"></a></h3>
<p>SSH key fingerprint is a security feature for easy identification/verification of the host, user is connecting to. This option allows to connect to the server without a password. On first connect, user is shown a fingerprint of a host-key, and asked if it should be added to the list of known hosts.</p>
<p>Please compare the fingerprint to the ones below, if one matches, the host can be added, if the fingerprint does not match, then there is a problem (e.g. man-in-the-middle-attack).<details><summary>SSH host keys of our servers</summary></p>
<p><strong>base.hpc.taltech.ee</strong></p>
<ul class="simple">
<li><p>ECDSA SHA256:OEfQiOB/eIG8hYoQ25sQk9T5tx9EtQbhi6sNM4C8mME</p></li>
<li><p>ED25519 SHA256:t0CSTU0AnSsJThzuM68tucrcfnn2wLKabjSnuRKX8Yc</p></li>
<li><p>RSA SHA256:qYrmOw/YN7wf640yBHADX3wnAOPu0OOXlcu4LKBxzG8</p></li>
</ul>
<p>.</p>
<p><strong>amp.hpc.taltech.ee</strong></p>
<ul class="simple">
<li><p>ECDSA SHA256:yl6+VaKow6qDZAXL3rQY8+3d3pcH0kYg7MjGgNVTWZs</p></li>
<li><p>ED25519 SHA256:YOjtpcEL2+AWm6vDFjVl0znYuQPMSVCkyFGvdO5fm8o</p></li>
<li><p>RSA SHA256:4aaOxumH1ATNfiIA4mZSNMefvxfdFm5zZoUj6VR7TYo</p></li>
</ul>
<p>.</p>
<p><strong>viz.hpc.taltech.ee</strong></p>
<ul class="simple">
<li><p>ECDSA SHA256:z2/bxleZ3T3vErkg4C7kvDPKKEU0qaoR8bL29EgMfGA</p></li>
<li><p>ED25519 SHA256:9zRBmS3dxD7BNISZKwg6l/2+6p4HeqlOhA4OMBjD9mk</p></li>
<li><p>RSA SHA256:Q6NDm88foRVTKtEAEexcRqPqMQNGUzf3rQdetBympPg</details></p></li>
</ul>
<p><a class="reference internal" href="ssh.html"><span class="doc">How to get SSH keys</span></a>.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
</section>
<section id="structure-and-file-tree">
<h2>Structure and file tree<a class="headerlink" href="#structure-and-file-tree" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>By accessing the cluster, the user gets into his home directory or <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> (<code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/home/$USER/</span></code>).</p>
<p>In the home directory, the user can create, delete, and overwrite files and perform calculations (if slurm script does not force program to use <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory). The home directory is limited in size of 500 GB and backups are performed once per week.</p>
<p>The home directory can be accessed from console or by GUI programs, but it cannot be mounted. For mounting was created special <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> and <code class="docutils literal notranslate"><span class="pre">smbgroup</span></code> folders (<code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/smbhome/$USER/</span></code> and <code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/smbgroup/</span></code>, respectively). More about <code class="docutils literal notranslate"><span class="pre">smb</span></code> folders can be found <a class="reference external" href="https://hpc.pages.taltech.ee/user-guides-newtest/quickstart.html#smb-cifs-exported-filesystems">here</a>.</p>
<p>Some programs and scripts suppose that files will be transfer to <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory at compute node and calculations will be done there. If job will be killed due to the time limit back transfer will not occur. In this case, user needs to know at which node this job was running (see <code class="docutils literal notranslate"><span class="pre">slurm-$job_id.stat</span></code>), to connect to exactly this node (in example it is green11). <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> directory will be in <code class="docutils literal notranslate"><span class="pre">/state/partition1/</span></code> and corresponds to jobID number.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">-</span><span class="n">w</span> <span class="n">green11</span> <span class="o">--</span><span class="n">pty</span> <span class="n">bash</span>
<span class="n">cd</span> <span class="o">/</span><span class="n">state</span><span class="o">/</span><span class="n">partition1</span><span class="o">/</span>
</pre></div>
</div>
<p>Please note that the scratch is <em>not</em> shared between nodes, so parallel MPI jobs that span multiple nodes cannot access each other’s scratch files.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="running-jobs-with-the-slurm">
<h2>Running jobs with the SLURM<a class="headerlink" href="#running-jobs-with-the-slurm" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>SLURM is a management and job scheduling system at Linux clusters. SLURM quick reference can be found <a class="reference external" href="https://slurm.schedmd.com/pdfs/summary.pdf">here</a>.</p>
<p>Examples of slurm scripts are usually given on the program’s page with some recommendations for optimal use of resources for this particular program. List of the programs installed at HPC is given on our <a class="reference internal" href="software.html"><span class="doc">software page</span></a>. At <a class="reference external" href="https://hpc.pages.taltech.ee/user-guides/software.html">software page</a> or program’s page also can be found information about licenses, since for programs installed at HPC have varying licence agreement. o use some licensed programs (for example, Gaussian), the user must be added to the appropriate group. For this contact us email (hpcsupport&#64;taltech.ee) or <a class="reference external" href="https://portal.taltech.ee/v2">Taltech portal</a>.</p>
<div class="simple1">
The most often used SLURM commands are:<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">srun</span></code> - to start a session or an application (in real time)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sbatch</span></code> - to start a computation using a batch file (submit for later execution)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">squeue</span></code> - to check the load of the cluster and status of own jobs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code> - to check the state of the cluster and partitions</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scancel</span></code> - to delete a submitted job (or stop a running one).</p></li>
</ul>
</div>
<br><p>For more parameters see the man-pages (manual) of the commands <code class="docutils literal notranslate"><span class="pre">srun</span></code>, <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> and <code class="docutils literal notranslate"><span class="pre">squeue</span></code>. For this use the command <code class="docutils literal notranslate"><span class="pre">man</span></code> followed by the program-name whose manual you want to see, e.g.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">man</span> <span class="n">srun</span>
</pre></div>
</div>
<p>Requesting resources with SLURM can be done either with parameters to <code class="docutils literal notranslate"><span class="pre">srun</span></code> or in a batch script invoked by <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<div class="simple1">The following defaults are used if not otherwise specified:<ul class="simple">
<li><p><strong>default memory</strong> – is 1 GB/thread (for larger jobs request more memory)</p></li>
<li><p><strong>short partition</strong> – <strong>default time limit</strong> is 10 min and  <strong>max time limit</strong> is 4 hours (longer jobs need to be submitted to partitions common or one of the infiniBand partitions)</p></li>
<li><p><strong>common partition</strong> –  <strong>default time</strong> is 10 min and <strong>max time limit</strong> is 8 days.</p></li>
<li><p><strong>long partition</strong> – <strong>default time</strong> is 10 min and <strong>time limit</strong> 15 days.</p></li>
<li><p><strong>green-ib partition</strong> – <strong>default time</strong> is 10 min and <strong>max time limit</strong> is 8 days</p></li>
<li><p><strong>gray-ib partition</strong> – <strong>default time</strong> is 10 min and <strong>max time limit</strong> is 8 days</p></li>
<li><p><strong>mem1tb partition</strong> – <strong>default time</strong> is 10 min and <strong>max time limit</strong> is 8 days</p></li>
<li><p><strong>gpu partition</strong> – <strong>default time</strong> is 10 min and <strong>max time limit</strong> is 5 days</p></li>
</ul>
 </div> 
 <br><p><strong>Running an interactive session</strong> longer than default 10 min. (here 1 hour):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">-</span><span class="n">t</span> <span class="mi">01</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span> <span class="o">--</span><span class="n">pty</span> <span class="n">bash</span> 
</pre></div>
</div>
<p>This logs you into one of the compute nodes, there you can load modules and run interactive applications, compile your code, etc.</p>
<p>With <code class="docutils literal notranslate"><span class="pre">srun</span></code> is reccomended to use CLI (command-line interface) instead of GUI (Graphical user interface) programs if it is possible. For example, use octave-CLI or octave instead of octave-GUI.</p>
<p><strong>Running a simple non-interactive single process job</strong> that lasts longer than default 4 hours (here 5 hours):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">--</span><span class="n">partition</span><span class="o">=</span><span class="n">common</span> <span class="o">-</span><span class="n">t</span> <span class="mi">05</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span> <span class="o">-</span><span class="n">n</span> <span class="mi">1</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
</pre></div>
</div>
<p><em><strong>NB!</strong></em> <em>Environment variables for OpenMP are <em>not</em> set automatically, e.g.</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span>  <span class="o">-</span><span class="n">N</span> <span class="mi">1</span> <span class="o">--</span><span class="n">cpus</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">task</span><span class="o">=</span><span class="mi">28</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
</pre></div>
</div>
<p>would <em>not</em> set <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> to 28, this has to be done manually. So usually, for parallel jobs it is recommended to use scripts for <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<p>Below is given an example of batch slurm script (filename: <code class="docutils literal notranslate"><span class="pre">myjob.slurm</span></code>) with explanation of the commands.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
#SBATCH --partition=common    ### Partition
#SBATCH --job-name=HelloOMP   ### Job Name           -J
#SBATCH --time=00:10:00       ### WallTime           -t
#SBATCH --nodes=4             ### Number of Nodes    -N 
#SBATCH --ntasks-per-node=7   ### Number of tasks (MPI processes)
#SBATCH --cpus-per-task=4     ### Number of threads per task (OMP threads)
#SBATCH --account=hpcrcf      ### In case of several accounts, specifies account used for job submission
#SBATCH --mem-per-cpu=100     ### Min RAM required in MB
#SBATCH --array=13-18         ### Array tasks for parameter sweep

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK		### setup environment
module load gcc						### setup environment
./hello_omp $SLURM_ARRAY_TASK_ID			### only for arrays, setup output files with system information
mpirun -n 28 ./hello_mpi 				### run program
</pre></div>
</div>
<p>In this example are listed some of the more common submission parameters. There are many more possible job-submission options, moreover, some of the options listed above  are not useful to apply together. An explanation of the variables used inside SLURM/SBATCH can be found <a class="reference external" href="https://slurm.schedmd.com/sbatch.html#lbAJ">here</a>. In contrast to e.g. GridEngine, SLURM allows fine-grained resource requests, using parameters like <code class="docutils literal notranslate"><span class="pre">--ntasks-per-core</span></code> or <code class="docutils literal notranslate"><span class="pre">--ntasks-per-node</span></code>.</p>
<div class="simple1"> 
 <a href="https://hpc.pages.taltech.ee/user-guides/slurm_example.html">An example script for submitting</a>:<ul class="simple">
<li><p>a single process job</p></li>
<li><p>an OpenMP parallel job</p></li>
<li><p>an MPI parallel job (OpenFOAM)</p></li>
<li><p>an array (parameter sweep) job</p></li>
<li><p>a GPU job</p></li>
<li><p>a job using the scratch partition (sequential or OpenMP parallel)</p></li>
</ul>
</div> 
<br><p>The job is then submitted to SLURM by</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="n">myjob</span><span class="o">.</span><span class="n">slurm</span>
</pre></div>
</div>
<p>and will be executed when the requested resources become available.</p>
<p>Output of applications and error messages are by default written to a <code class="docutils literal notranslate"><span class="pre">slurm-$job_id.out</span></code> file. More about SLURM finished job statistics can be found <a class="reference internal" href="slurm_statistics.html"><span class="doc">here</span></a>.</p>
<div class="simple1">
Some useful online resources:<ul class="simple">
<li><p><a class="reference external" href="https://slurm.schedmd.com/pdfs/summary.pdf">SLURM scheduler workload manager</a></p></li>
<li><p>Victor Eijkhout: Introduction to High-Performance Scientific Computing <a href="https://doi.org/10.5281/zenodo.49897"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.49897.svg" alt="DOI"></a></p></li>
<li><p><a class="reference external" href="http://cnx.org/content/col11136/1.5/">Charles Severance, Kevin Dowd: High Performance Computing</a></p></li>
<li><p><a class="reference external" href="https://www.openmp.org/">OpenMP standard</a></p></li>
<li><p><a class="reference external" href="https://www.mpi-forum.org/">MPI standard</a></p></li>
<li><p><a class="reference external" href="https://slurm.schedmd.com/pdfs/summary.pdf">SLURM Quick Reference (Cheat Sheet)</a></p></li>
</ul>
 </div>  <br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="slurm-accounts">
<h2>SLURM accounts<a class="headerlink" href="#slurm-accounts" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>In SLURM exist accounts for billing, these are different from the login account!</p>
<p>Each user has his/her own personal SLURM-account, which will have a monthly limit and at least one project account for larger calculations.</p>
<p>SLURM user-accounts start with <code class="docutils literal notranslate"><span class="pre">user_</span></code> and project accounts with <code class="docutils literal notranslate"><span class="pre">project_</span></code> and course accounts with <code class="docutils literal notranslate"><span class="pre">course_</span></code>, followed by uniID/projectID/courseID. You can check which SLURM accounts you belong to, by <code class="docutils literal notranslate"><span class="pre">sacctmgr</span> <span class="pre">show</span> <span class="pre">associations</span> <span class="pre">format=account%30,user%30</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">uniID</span></code> . Currently (almost) all users belong to the SLURM-account “vaikimisi” (default), it is possible to submit jobs under this account, especially if no <code class="docutils literal notranslate"><span class="pre">user_</span></code> or project account has been created for you yet, however, “vaikimisi” will be discontinued in the near future.</p>
<p>When submitting a job, it is important to use the correct SLURM-account <code class="docutils literal notranslate"><span class="pre">--account=SLURM-ACCOUNT</span></code>, as this is connected to the financial source.</p>
<br> 
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
<section id="monitoring-jobs-resources">
<h2>Monitoring jobs &amp; resources<a class="headerlink" href="#monitoring-jobs-resources" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<section id="monitoring-a-job-on-the-node">
<h3>Monitoring a job on the node<a class="headerlink" href="#monitoring-a-job-on-the-node" title="Permalink to this heading"></a></h3>
<section id="status-of-a-job">
<h4>Status of a job<a class="headerlink" href="#status-of-a-job" title="Permalink to this heading"></a></h4>
<p>User can check the status his jobs (whether they are running or not, and on which node) by the command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>squeue -u $USER
</pre></div>
</div>
<p><img alt="squeue" src="_images/squeue.png" /></p>
</section>
<section id="load-of-the-node">
<h4>Load of the node<a class="headerlink" href="#load-of-the-node" title="Permalink to this heading"></a></h4>
<p>User can check the load of the node his job runs on, status and configuration of this node by command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scontrol</span> <span class="n">show</span> <span class="n">node</span> <span class="o">&lt;</span><span class="n">nodename</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>the load should not exceed the number of hyperthreads (CPUs in SLURM notation) of the node.</p>
<p><img alt="scontrol" src="_images/scontrol.png" /></p>
<p>In case of MPI parallel runs statistics of several nodes can be monitored by specifying nodes names. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scontrol</span> <span class="n">show</span> <span class="n">node</span><span class="o">=</span><span class="n">green</span><span class="p">[</span><span class="mi">25</span><span class="o">-</span><span class="mi">26</span><span class="p">]</span>
</pre></div>
</div>
<p>Node features for node selection using <code class="docutils literal notranslate"><span class="pre">--constraint=</span></code>:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>feature</th>
<th>what it is</th>
</tr>
</thead>
<tbody>
<tr>
<td>A100-40</td>
<td>has A100 GPU with 40GB</td>
</tr>
<tr>
<td>A100-80</td>
<td>has A100 GPU with 80GB</td>
</tr>
<tr>
<td>L40</td>
<td>has L40 GPU with 48GB</td>
</tr>
<tr>
<td>sm80</td>
<td>GPU has compute capability 8.0 (A100, L40)</td>
</tr>
<tr>
<td>sm89</td>
<td>GPU has compute capability 8.9  (L40)</td>
</tr>
<tr>
<td>sm35</td>
<td>GPU has compute capability 3.5 (K20Xm, A100, L40)</td>
</tr>
<tr>
<td>zen</td>
<td>AMD Zen CPU architecture (amp1)</td>
</tr>
<tr>
<td>zen3</td>
<td>AMD Zen CPU architecture 3rd generation (amp2)</td>
</tr>
<tr>
<td>zen4</td>
<td>AMD Zen CPU architecture 4th generation (ada*)</td>
</tr>
<tr>
<td>avx512</td>
<td>CPU has avx512 (skylake, zen4)</td>
</tr>
<tr>
<td>skylake</td>
<td>Intel SkyLake CPU architecture (green*)</td>
</tr>
<tr>
<td>sandybridge</td>
<td>Intel SandyBridge CPU architecture (mem1tb, viz)</td>
</tr>
<tr>
<td>ib</td>
<td>InfiniBand network interface</td>
</tr>
</tbody>
</table></section>
<section id="monitoring-with-interactive-job">
<h4>Monitoring with interactive job<a class="headerlink" href="#monitoring-with-interactive-job" title="Permalink to this heading"></a></h4>
<p>It is possible to submit a second interactive job to the node where the main job is running, check with <code class="docutils literal notranslate"><span class="pre">squeue</span></code> where your job is running, then submit</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">-</span><span class="n">w</span> <span class="o">&lt;</span><span class="n">nodename</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">pty</span> <span class="n">htop</span>
</pre></div>
</div>
<p>Note that there must be free slots on the machine, so if you cannot use <code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">80</span></code> or <code class="docutils literal notranslate"><span class="pre">--exclusive</span></code> for your main job (use <code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">79</span></code>).</p>
<p><img alt="htop" src="_images/htop.png" /></p>
<p>Press <code class="docutils literal notranslate"><span class="pre">q</span></code> to exit.</p>
<details><summary>You can also add a column that shows the CPU number of the program (for more details click here).</summary><p>For Linux <strong>F1-F10</strong> keys should be used, for <strong>Mac</strong> - just click on the corresponding buttons.</p>
<p><img alt="htop-1" src="_images/htop-1.png" /></p>
<p>Will appear a new column, showing the CPU number of the program.</p>
<p><img alt="htop-2" src="_images/htop-2.png" /></p>
</details>   <br></section>
<section id="monitoring-jobs-using-gpus">
<h4>Monitoring jobs using GPUs<a class="headerlink" href="#monitoring-jobs-using-gpus" title="Permalink to this heading"></a></h4>
<p>Log to <strong>amp</strong> or <strong>amp2</strong>. Command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>echo ${SLURM_STEP_GPUS:-$SLURM_JOB_GPUS} 
</pre></div>
</div>
<p>shows the GPU IDs allocated to your job.</p>
<p>GPUs load can be checked by command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span>
</pre></div>
</div>
<p><img alt="nvidia-smi" src="_images/nvidia-smi.png" /></p>
<p>Press <code class="docutils literal notranslate"><span class="pre">control+c</span></code> to exit.</p>
<p>Another option is to logging to <strong>amp</strong> or <strong>amp2</strong>, check which GPUs are allocated to your job, and give command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">nvtop</span>
</pre></div>
</div>
<p><img alt="nvtop" src="_images/nvtop.png" /></p>
<p>Press <code class="docutils literal notranslate"><span class="pre">q</span></code> to exit.</p>
<p>An alternative method <strong>on Linux computers,</strong> if you have X11. Logging to <strong>base/amp</strong> with <code class="docutils literal notranslate"><span class="pre">--X</span></code> key:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ssh</span> <span class="o">--</span><span class="n">X</span> <span class="n">UniID</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>
</pre></div>
</div>
<p>then submit your main interactive job</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">--</span><span class="n">x11</span> <span class="o">-</span><span class="n">n</span> <span class="o">&lt;</span><span class="n">numtasks</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">cpus</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">task</span><span class="o">=&lt;</span><span class="n">numthreads</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">pty</span> <span class="n">bash</span>
</pre></div>
</div>
<p>and start an <code class="docutils literal notranslate"><span class="pre">xterm</span> <span class="pre">-e</span> <span class="pre">htop</span> <span class="pre">&amp;</span></code> in the session.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> the option <code class="docutils literal notranslate"><span class="pre">--x11=batch</span></code> can be used, note that the ssh session to <strong>base</strong> needs to stay open!</p>
</section>
</section>
<section id="monitoring-resource-usage">
<h3>Monitoring resource usage<a class="headerlink" href="#monitoring-resource-usage" title="Permalink to this heading"></a></h3>
<p>Default disc quota for <code class="docutils literal notranslate"><span class="pre">home</span></code> (that is backed up weekly) is 500 GB and for <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> (that is not backed up) – 2 TB per user. For <code class="docutils literal notranslate"><span class="pre">smbgroup</span></code> there is no limits and no backup.</p>
<p>The easiest way to check your current disk usage is to look at the table that appears when you log in to HPC.</p>
<p><img alt="disk_usage" src="_images/disk_usage.png" /></p>
<p>You can also monitor your resource usage by <code class="docutils literal notranslate"><span class="pre">taltech-lsquota.bash</span></code> script and <code class="docutils literal notranslate"><span class="pre">sreport</span></code> command.</p>
<p>Current disk usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">taltech</span><span class="o">-</span><span class="n">lsquota</span><span class="o">.</span><span class="n">bash</span>
</pre></div>
</div>
<p><img alt="usage" src="_images/usage2.png" /></p>
<p>CPU usage during last day:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>sreport -t Hours cluster UserUtilizationByAccount Users=$USER
</pre></div>
</div>
<p><img alt="CPUhours" src="_images/CPUhours.png" /></p>
<p>CPU usage in specific period (e.g. since beginning of this year):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>sreport -t Hours cluster UserUtilizationByAccount Users=$USER start=2024-01-01T00:00:00 end=2024-12-31T23:59:59
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">start=</span></code> and <code class="docutils literal notranslate"><span class="pre">end=</span></code> can be changed depending on the desired period of time.</p>
<p><img alt="CPUhours_1" src="_images/CPUhours1.png" /></p>
<p>For convenience, a tool <code class="docutils literal notranslate"><span class="pre">taltech-history</span></code> was created, by default it shows the jobs of the current month, use <code class="docutils literal notranslate"><span class="pre">taltech-history</span> <span class="pre">-a</span></code> to get a summary of the useh hours and costs of the current month.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
</section>
<section id="copying-data-to-from-the-clusters">
<h2>Copying data to/from the clusters<a class="headerlink" href="#copying-data-to-from-the-clusters" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>Since HPC disk quota is limited, it is recommended to have your own copy of important calculations and results. Data from HPC can be transferred by several commands: <code class="docutils literal notranslate"><span class="pre">scp</span></code>, <code class="docutils literal notranslate"><span class="pre">sftp</span></code>, <code class="docutils literal notranslate"><span class="pre">sshfs</span></code> or <code class="docutils literal notranslate"><span class="pre">rsync</span></code>.</p>
<ol>
<li><p><code class="docutils literal notranslate"><span class="pre">scp</span></code> is available on all <strong>Linux systems,</strong> <strong>Mac</strong> and <strong>Windows10 PowerShell.</strong> There are also GUI versions available for different OS (like <a class="reference internal" href="putty.html"><span class="doc">PuTTY</span></a>).</p>
<p>Copying <em><strong>to</strong></em> the cluster with <code class="docutils literal notranslate"><span class="pre">scp</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">local_path_from_where_copy</span><span class="o">/</span><span class="n">file</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="p">:</span><span class="n">path_where_to_save</span>
</pre></div>
</div>
<p><img alt="scp" src="_images/scp1.png" /></p>
<p>Copying <em><strong>from</strong></em> the cluster with <code class="docutils literal notranslate"><span class="pre">scp</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="p">:</span><span class="n">path_from_where_copy</span><span class="o">/</span><span class="n">file</span> <span class="n">local_path_where_to_save</span> 
</pre></div>
</div>
<p><img alt="scp" src="_images/scp2.png" /></p>
<p>Path to the file at HPC can be checked by  <code class="docutils literal notranslate"><span class="pre">pwd</span></code> command.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">sftp</span></code> is the secure version of the <code class="docutils literal notranslate"><span class="pre">ftp</span></code> protocol vailable on <strong>Linux,</strong> <strong>Mac</strong> and <strong>Windows10 PowerShell.</strong> This command starts a session, in which files can be transmitted in both directions using the <code class="docutils literal notranslate"><span class="pre">get</span></code> and <code class="docutils literal notranslate"><span class="pre">put</span></code> commands. File transfer can be done in “binary” or “ascii” mode, conversion of line-endings (see below) is automatic in “ascii” mode. There are also GUI versions available for different OS (<a class="reference external" href="https://filezilla-project.org/">FileZilla</a>, <a class="reference external" href="https://github.com/masneyb/gftp">gFTP</a> and <a class="reference external" href="https://winscp.net/eng/index.php">WinSCP</a> (Windows))</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sftp</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>
</pre></div>
</div>
<p><img alt="sftp" src="_images/sftp.png" /></p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">sshfs</span></code> can be used to temporarily mount remote filesystems for data transfer or analysis. Available in <strong>Linux.</strong> The data is tunneled through an ssh-connection. Be sware that this is usually not performant and can creates high load on the login node due to ssh-encryption.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sshfs</span> <span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="nd">@base</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="p">:</span><span class="n">remote_dir</span><span class="o">/</span> <span class="o">/</span><span class="n">path_to_local_mount_point</span><span class="o">/</span>
</pre></div>
</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">rsync</span></code> can update files if previous versions exist without having to transfer the whole file. However, its use is recommended <strong>for the advanced user only</strong> since one has to be careful with the syntax.</p></li>
</ol>
<section id="smb-cifs-exported-filesystems">
<h3>SMB/CIFS exported filesystems<a class="headerlink" href="#smb-cifs-exported-filesystems" title="Permalink to this heading"></a></h3>
<p>One of the simple and convenient ways to control and process data based on HPC is mounting. Mounting means that user attaches his directory placed at HPC to a directory on his computer and can process files as if they were on this computer. These can be accessed from within university or from <a class="reference external" href="https://eduvpn.taltech.ee/vpn-user-portal/home">EduVPN</a>.</p>
<p>Each user automatically has a directory within <code class="docutils literal notranslate"><span class="pre">smbhome</span></code>. It does not match with <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> directory, so calculations should be initially done at <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> directory to prevent copying or files needed should be copied from <code class="docutils literal notranslate"><span class="pre">home</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">smbhome</span></code> directory by commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>pwd	### look path to the file 
cp path_to_your_file/your_file /gpfs/mariana/smbhome/$USER/	### copying
</pre></div>
</div>
<p>To get a directory for group access, please contact us (a group and a directory need to be created).</p>
<p>The HPC center exports two filesystems as Windows network shares:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>local path on cluster</th>
<th>Linux network URL</th>
<th>Windows network URL</th>
</tr>
</thead>
<tbody>
<tr>
<td>/gpfs/mariana/smbhome/$USER</td>
<td>smb://smb.hpc.taltech.ee/smbhome</td>
<td>\\smb.hpc.taltech.ee\smbhome</td>
</tr>
<tr>
<td>/gpfs/mariana/smbgroup</td>
<td>smb://smb.hpc.taltech.ee/smbgroup</td>
<td>\\smb.hpc.taltech.ee\smbgroup</td>
</tr>
<tr>
<td>/gpfs/mariana/home/$USER</td>
<td>not exported</td>
<td>not exported</td>
</tr>
</tbody>
</table><p><strong>This is the quick-access guide, for more details, see <a class="reference external" href="samba.html">here</a></strong></p>
<section id="windows-access">
<h4>Windows access<a class="headerlink" href="#windows-access" title="Permalink to this heading"></a></h4>
<p>The shares can be found using the Explorer “Map Network Drive”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">server</span> <span class="o">&gt;&gt;&gt;</span> \\<span class="n">smb</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>\<span class="n">smbhome</span>
<span class="n">username</span> <span class="o">&gt;&gt;&gt;</span> <span class="n">INTRA</span>\<span class="o">&lt;</span><span class="n">uni</span><span class="o">-</span><span class="nb">id</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>From Powershell:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="n">net</span> <span class="n">use</span> \\<span class="n">smb</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span>\<span class="n">smbhome</span> <span class="o">/</span><span class="n">user</span><span class="p">:</span><span class="n">INTRA</span>\<span class="n">uni</span><span class="o">-</span><span class="nb">id</span>
 <span class="n">get</span><span class="o">-</span><span class="n">smbconnection</span>
</pre></div>
</div>
</section>
<section id="linux-access">
<h4>Linux access<a class="headerlink" href="#linux-access" title="Permalink to this heading"></a></h4>
<p>On Linux with GUI Desktop, the shares can be accessed with the nautilus browser.</p>
<p>From commandline, the shares can be mounted as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dbus</span><span class="o">-</span><span class="n">run</span><span class="o">-</span><span class="n">session</span> <span class="n">bash</span>
<span class="n">gio</span> <span class="n">mount</span> <span class="n">smb</span><span class="p">:</span><span class="o">//</span><span class="n">smb</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="o">/</span><span class="n">smbhome</span><span class="o">/</span>
</pre></div>
</div>
<p>you will be asked for “User” (which is your UniID), “Domain” (which is “INTRA”), and your password.</p>
<p>To disconnect from the share, unmount with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gio</span> <span class="n">mount</span> <span class="o">-</span><span class="n">u</span> <span class="n">smb</span><span class="p">:</span><span class="o">//</span><span class="n">smb</span><span class="o">.</span><span class="n">hpc</span><span class="o">.</span><span class="n">taltech</span><span class="o">.</span><span class="n">ee</span><span class="o">/</span><span class="n">smbhome</span><span class="o">/</span>
</pre></div>
</div>
</section>
</section>
<section id="special-considerations-for-copying-windows-linux">
<h3>Special considerations for copying Windows - Linux<a class="headerlink" href="#special-considerations-for-copying-windows-linux" title="Permalink to this heading"></a></h3>
<p>Microsoft Windows is using a different line ending in text files (ASCII/UTF8 files) than Linux/Unix/Mac: CRLF vs. LF
When copying files between Windows-Linux, this needs to be taken into account. The FTP (File Transfer Protocol) has ASCII and BINARY modes, in ASCII-mode the line-end conversion is automatic.</p>
<p>There are tools for conversion of the line-ending, in case the file was copied without line conversion: <code class="docutils literal notranslate"><span class="pre">dos2unix</span></code>, <code class="docutils literal notranslate"><span class="pre">unix2dos</span></code>, <code class="docutils literal notranslate"><span class="pre">todos</span></code>, <code class="docutils literal notranslate"><span class="pre">fromdos</span></code>, the stream-editor <code class="docutils literal notranslate"><span class="pre">sed</span></code> can also be used.</p>
<br>
<br>
<hr style="margin-right: 0px; margin-bottom: 4px; margin-left: 0px; margin-top: -24px; border:2px solid  #d9d9d9 "></hr>
<hr style="margin: 4px 0px; border:1px solid  #d9d9d9 "></hr></section>
</section>
<section id="backup">
<h2>Backup<a class="headerlink" href="#backup" title="Permalink to this heading"></a></h2>
<hr class="docutils" />
<p>There are 2 major directories where users can store data:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/home/</span></code> default home directory which is limited to 500GB and is backed up, excluding specific directories: <code class="docutils literal notranslate"><span class="pre">[*/envs/,</span> <span class="pre">*/.cache/,</span> <span class="pre">*/pkgs/]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/gpfs/mariana/smbhome/</span></code> has a limit of 2TB and is not backed up.</p></li>
</ul>
<p>The home directory is meant for critical data like configurations and scripts, whereas smbhome is meant for data.</p>
<p>The backup will run weekly. <em>If the home directory is larger than 500GB [usage is displayed upon login to the cluster] it will not be backed up.</em></p>
<p>If your home directory is larger than 500G please move the data to smbhome.</p>
<hr class="docutils" />
<p>At HPC are installed programs with varying licence agreement. To use some licensed programs (for example, Gaussian), the user must be added to the appropriate group. For this contact us email (hpcsupport&#64;taltech.ee) or <a class="reference external" href="https://portal.taltech.ee/v2">Taltech portal</a>. More about available programs and licenses can be found at <a class="reference external" href="https://hpc.pages.taltech.ee/user-guides/software.html">software page</a>.</p>
<br></section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cloud.html" class="btn btn-neutral float-left" title="Quickstart: Cloud" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="learning.html" class="btn btn-neutral float-right" title="Courses and introductions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright COPYRIGHT.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>